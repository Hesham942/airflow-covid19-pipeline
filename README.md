# COVID-19 Case Data Cleansing & Modeling with Apache Airflow

## üöÄ Project Overview
I have recently completed my learning journey in **Apache Airflow** and applied my knowledge by working on a project: **COVID-19 Case Data Cleansing & Modeling**. This project is part of my hands-on learning approach to ensure I am on the right track while reinforcing my skills through practical implementation.

## üìå Project Goals
- Ingest raw COVID-19 case data.
- Cleanse and preprocess the dataset.
- Transform the data into a structured format suitable for analysis.
- Automate the workflow using Apache Airflow.

## üîß Tech Stack
- **Apache Airflow** (for workflow automation)
- **Docker** (for containerized execution)
- **PostgreSQL** (for data storage)
- **Pandas** (for data manipulation)

## üìÇ Repository Structure
```
üì¶ airflow-covid19-data-pipeline
 ‚î£ üìÇ dags                 # Airflow DAGs (data pipeline workflows)
 ‚î£ üìÇ scripts              # Custom scripts for data processing
 ‚î£ üìÇ logs                 # Airflow logs
 ‚î£ üìÇ plugins              # Airflow plugins (if any)
 ‚î£ üìÑ .env                 # Environment variables (Airflow image & UID)
 ‚î£ üìÑ airflow.cfg          # Airflow configuration file
 ‚î£ üìÑ docker-compose.yaml  # Docker configuration for Airflow setup
 ‚îó üìÑ README.md            # Project documentation
```

## üõ† How to Run the Project Locally
If you want to test or run this project on your own machine, follow these steps:

### 1Ô∏è‚É£ Clone the Repository
```bash
git clone https://github.com/hesham942/airflow-covid19-data-pipeline.git
cd airflow-covid19-data-pipeline
```

### 2Ô∏è‚É£ Set Up Airflow
Make sure you have **Docker** installed and running. Then, initialize Airflow using:
```bash
echo -e "AIRFLOW_UID=$(id -u)" > .env
docker-compose up airflow-init
```

### 3Ô∏è‚É£ Start the Airflow Services
Run the following command to start Airflow and its components:
```bash
docker-compose up
```
This will start the **PostgreSQL**, **Redis**, and **Airflow webserver, scheduler, and worker**.

### 4Ô∏è‚É£ Access the Airflow Web UI
Once the setup is complete, you can access the **Airflow Web UI** by visiting:
```
http://localhost:8080
```
Login with the default credentials:
- **Username:** `airflow`
- **Password:** `airflow`

### 5Ô∏è‚É£ Trigger the DAG
In the Airflow UI, enable and trigger the **COVID-19 Case Data Pipeline DAG** to start the workflow.

## üìà Expected Output
- The pipeline fetches COVID-19 case data.
- Cleans and structures the data.
- Stores the transformed data in PostgreSQL for further analysis.

## üèÜ Key Takeaways
- **Gained hands-on experience** with Airflow DAGs and task dependencies.
- **Improved data engineering skills** by automating ETL pipelines.
- **Built a reusable Airflow setup** using Docker and PostgreSQL.

---
